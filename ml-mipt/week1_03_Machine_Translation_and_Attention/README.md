Neural Machine Translation as seq2sec:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/advanced_f20/week1_03_Machine_Translation_and_Attention/practice_seq2seq_NMT.ipynb)

Attention basics and Tensorboard example:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/advanced_f20/week1_03_Machine_Translation_and_Attention/extra_practice_Attention_basics_and_tensorboard.ipynb)


Further readings:

* Great blog post by Jay Alammar: https://jalammar.github.io/illustrated-transformer/
* Notebook on positional encoding: [link](https://github.com/ml-mipt/ml-mipt/blob/advanced/week04_Transformer/week04_positional_encoding_carriers.ipynb)
* Great Annotated Transformer article with code and comments by Harvard NLP group: https://nlp.seas.harvard.edu/2018/04/03/attention.html


## Machine Translation and Attention

- [ml-mipt_f20_lect103_Machine_Tranlation.pdf](../week1_03_Machine_Translation_and_Attention/ml-mipt_f20_lect103_Machine_Tranlation.pdf)
- [practice_seq2seq_NMT.ipynb](../week1_03_Machine_Translation_and_Attention/practice_seq2seq_NMT.ipynb)
- [extra_practice_Attention_basics_and_tensorboard.ipynb](../week1_03_Machine_Translation_and_Attention/extra_practice_Attention_basics_and_tensorboard.ipynb)


- Архитектура Seq2Seq(Encoder + Decoder)
![](../for_readme/week1_03_Machine_Translation_and_Attention/1.png)
![](../for_readme/week1_03_Machine_Translation_and_Attention/2.png)
- Beam search
![](../for_readme/week1_03_Machine_Translation_and_Attention/3.png)
- Метрика качества машинного перевода
![](../for_readme/week1_03_Machine_Translation_and_Attention/4.png)
- Attention
![](../for_readme/week1_03_Machine_Translation_and_Attention/5.png)
![](../for_readme/week1_03_Machine_Translation_and_Attention/6.png)
- Варианты реализации Attention
![](../for_readme/week1_03_Machine_Translation_and_Attention/7.png)

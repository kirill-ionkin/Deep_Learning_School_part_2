## Transformers

- [Transformers.pdf](Transformers.pdf)
- [Attention_is_All_You_Need.ipynb](Attention_is_All_You_Need.ipynb)


- [Лекция. Трансформеры. Self-Attention](https://www.youtube.com/watch?v=f544TipD2QY)
- [Лекция. Трансформеры. Positional Encoding, Decoder side, Training](https://www.youtube.com/watch?v=WL6UviEG3XA)
- [Семинар. Трансформеры](https://www.youtube.com/watch?v=KdMS5XWQAic)


- Архитектура transformers
![](../for_readme/12_Transformers/1.png)
- Более детальная архитектура transformers
![](../for_readme/12_Transformers/2.png)
- Архитектура Encoder
![](../for_readme/12_Transformers/3.png)
- Визуализация Механизма Self-Attention
![](../for_readme/12_Transformers/4.png)
- Self-Attention
![](../for_readme/12_Transformers/5.png)
![](../for_readme/12_Transformers/6.png)
- Self-Attention(через матрицы)
![](../for_readme/12_Transformers/7.png)
![](../for_readme/12_Transformers/8.png)
- Multi-Head Attention
![](../for_readme/12_Transformers/9.png)
![](../for_readme/12_Transformers/10.png)
- Конкатенация выводов Multi-head Attention
![](../for_readme/12_Transformers/11.png)
![](../for_readme/12_Transformers/12.png)
- Attention vs. Multi-Head Attention
![](../for_readme/12_Transformers/13.png)
- Positional Encoding
![](../for_readme/12_Transformers/14.png)
![](../for_readme/12_Transformers/15.png)
![](../for_readme/12_Transformers/16.png)
- Layer Normalization
![](../for_readme/12_Transformers/17.png)
- Decoder
![](../for_readme/12_Transformers/18.png)
![](../for_readme/12_Transformers/19.png)
![](../for_readme/12_Transformers/20.png)
- Training transformers
![](../for_readme/12_Transformers/21.png)
